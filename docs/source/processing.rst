Processing
==========

Derivatives are generated in Arcana by "pipelines", modular Pydra workflows
that connect dataset columns (see :ref:`data_columns`). Pipeline outputs
are connected to sink columns, while pipeline inputs can draw data from either
source columns or sink columns containing derivatives generated by prerequisite
pipelines.


All processing is performed by the Pydra_ workflow engine, which defines
a declarative domain-specific language enables tasks
to be spread over multiple cores or submitted to job-schedulers typically used
by high-performance clusters (i.e. SLURM and SGE).


Pipelines
---------

Pipeline objects wrap around an "inner" workflow, which can consist of a
single Pydra task, or a Pydra workflow containing several tasks, that
processes/analyses the data. An "outer" workflow is implicitly created by the
pipeline object, which adds tasks to iterate over the relevant nodes in the
dataset, handle storage and retrieval to and from the data store,
convert data between different file formats and ensure that previously
generated derivatives were generated with equivalent parameterisations
and software versions.

To add a pipeline to a dataset via the API use the ``Dataset.pipeline()`` method

.. code-block:: python

    from pydra.tasks.freesurfer import Freesurfer
    from arcana.data.formats import common, medicalimaging

    dataset = Dataset.load('myuni-xnat//myproject:training')

    dataset.add_source('T1w', medicalimaging.dicom, path='.*mprage.*',
                       is_regex=True)
    dataset.add_source('T2w', medicalimaging.dicom, path='.*t2spc.*',
                       is_regex=True)

    dataset.add_sink('freesurfer/recon-all', common.directory)

    dataset.pipeline(
        name='freesurfer,
        workflow=Freesurfer(
            param1=10.0,
            param2=20.0),
        inputs=[('T1w', 'in_file', medicalimaging.nifti_gz),
                ('T2w', 'peel', medicalimaging.nifti_gz)],
        outputs=[('freesurfer/recon-all', 'out_file', common.directory)])

Alternatively, if the source can be referenced by its path alone you can
add the sources and sinks in one step

.. code-block:: python

    from pydra.tasks.fsl.preprocess.fast import FAST
    from arcana.data.formats import common, medicalimaging

    dataset = Dataset.load('file///data/openneuro/ds00014:test')

    dataset.pipeline(
        name='segmentation',
        workflow=FAST(
            method='a-method'),
        sources=[('T1w', 'in_file', medicalimaging.nifti_gz)],
        sinks=[('fast/gm', 'gm', medicalimaging.nifti_gz)])

    # Save pipeline to dataset metadata for subsequent reuse.
    dataset.save()


To connect a workflow via the CLI

.. code-block:: bash

    $ arcana column add-source 'myuni-xnat//myproject:training' T1w \
      medicalimaging:dicom --path '.*mprage.*'
    $ arcana column add-source 'myuni-xnat//myproject:training' T2w \
      medicalimaging:dicom --path '.*t2spc.*'
    $ arcana pipeline 'myuni-xnat//myproject:training' freesurfer \
      pydra.tasks.freesurfer:Freesurfer \
      --input T1w in_file medicalimaging:nifti_gz \
      --input T2w peel medicalimaging:nifti_gz \
      --output freesurfer/recon-all out_file common:directory \
      --parameter param1 10.0 \
      --parameter param2 20.0

Adding sinks and sources in one step where they can be specified by their
path and datatype alone looks like

.. code-block:: bash

    $ arcana pipeline 'file///data/openneuro/ds00014:test' segmentation \
      pydra.tasks.fsl.preprocess.fast:FAST \
      --source T1w in_file medicalimaging:nifti_gz \
      --sink fast/gm gm medicalimaging:nifti_gz \
      --parameter method a-method


By default, pipelines will iterate the most-frequent nodes in the dataset
(i.e. the leaf nodes of the data tree). However, pipelines can be run over
any frequency, e.g. per subject, per timepoint, or on the dataset as a
whole (to create common templates/statistics). For example, the pipeline
in the following code-block will only run once per dataset.


.. code-block:: python

    from myworkflows import vbm_template
    from arcana.data.formats import common, medicalimaging
    from arcana.data.spaces.medicalimaging import ClinicalTrial

    dataset = Dataset.load('file///data/openneuro/ds00014:test')

    dataset.pipeline(
        name='vbm_template',
        workflow=vbm_template(),
        sources=[('T1w', 'in_file', medicalimaging.nifti_gz)],
        sinks=[('vbm_template', 'out_file', medicalimaging.nifti_gz)],
        frequency=ClinicalTrial.dataset)


Derivatives
-----------

After data sinks have been defined and connected to a pipeline, they can be
generated using the ``derive`` method on the dataset. This method checks the
dataset to see whether the source data is present and executes the
pipelines over all nodes of the dataset with available source data by default.
If pipeline inputs are sink columns to be derived by prerequisite pipelines,
then the prerequisites are executed first.


To generate derivatives via the API

.. code-block:: python

  dataset = Dataset.load('file///data/openneuro/ds00014:test')

  dataset.derive('fast/gm', work_dir='/work/temp-dir')

  # Print URI of generated dataset
  print(dataset['fast/gm']['sub11'].uri)


To generate derivatives via the CLI

.. code-block:: bash

  $ arcana derive 'myuni-xnat//myproject:training' freesurfer/recon-all


Under the hood, Arcana uses the Pydra workflow engine to execute the pipelines.
By default it will use the Pydra's "cf" plugin, which uses "concurrent-futures"
to split workflows over multiple processes. You can specify which plugin, and
thereby how the workflow is executed via the ``pydra_plugin`` option, and pass
options to it with ``pydra_option``.


.. code-block:: bash

  $ arcana derive 'myuni-xnat//myproject:training' freesurfer/recon-all \
    --pydra_plugin slurm --pydra_option poll_delay 5 --pydra_option max_jobs 10


Analysis classes
----------------

.. warning::

  Under construction


Provenance
----------

Provenance metadata is saved alongside derivatives in the data store. The
metadata includes:

* MD5 Checksums of all pipeline inputs and outputs
* Full workflow graph with connections between, and parameterisations of, Pydra tasks
* Container image tags for tasks that ran inside containers
* Python dependencies and versions used.

How these provenance metadata are stored will depend on the type data store,
but often it will be stored in a JSON file. For example, a provenance JSON file
would look like

.. code-block:: javascript

  {
    "store": {
      "type": "xnat",
      "location": "https://central.xnat.org"
    },
    "dataset": {
      "id": "MYPROJECT",
      "name": "training"
    },
    "checksums": {
      "inputs": {
        // MD5 Checksums for all files in the file group. "." refers to the
        // "primary file" in the file group.
        "T1w_reg_dwi": {
          ".": "4838470888DBBEADEAD91089DD4DFC55",
          "json": "7500099D8BE29EF9057D6DE5D515DFFE"
        },
        "T2w_reg_dwi": {
          ".": "4838470888DBBEADEAD91089DD4DFC55",
          "json": "5625E881E32AE6415E7E9AF9AEC59FD6"
        },
        "dwi_fod": {
          ".": "92EF19B942DD019BF8D32A2CE2A3652F"
        }
      },
      "outputs": {
        "wm_tracks": {
          ".": "D30073044A7B1239EFF753C85BC1C5B3"
        }
      }
    },
    "pipeline": {
      "name": "anatomically_constrained_tractography",
      // List all tasks in the pipeline and the inputs to them. 
      "tasks": [
        {
          "name": "5ttgen",
          "task": {
            "module": "pydra.tasks.mrtrix3.preprocess",
            "name": "FiveTissueTypes",
            "package": "pydra-mrtrix",
            "version": "0.1.1"
          }
          "inputs": {
            "in_file": {
              "field": "T1w_reg_dwi"
            }
            "t2": {
              "field": "T1w_reg_dwi"
            }
            "sgm_amyg_hipp": true
          },
          "image": {
            "type": "docker",
            "tag": "mrtrix3/mrtrix3"
          }
        },
        {
          "name": "tckgen",
          "task": {
            "module": "pydra.tasks.mrtrix3.tractography",
            "name": "TrackGen",
            "package": "pydra-mrtrix",
            "version": "0.1.1"
          }
          "inputs": {
            "in_file": {
              "field": "dwi_fod"
            },
            "act": {
              "task": "5ttgen",
              "field": "out_file"
            },
            "select": 100000000,
          },
          "image": {
            "type": "docker",
            "tag": "mrtrix3/mrtrix3"
          }
        }
      ],
      "outputs": {
        "wm_tracks": {
          "task": "tckgen",
          "field": "out_file"
        }
      }
    }
  }


Before derivatives are generated, provenance metadata of prerequisite
derivatives (i.e. inputs of the pipeline and prerequisite pipelines, etc...)
are checked to see if there have been any alterations to the configuration of
the pipelines that generated them. If so, any affected nodes will not be
processed, and a warning will be generated, unless the ``reprocess`` flag is
set

.. code-block:: python

  dataset.derive('fast/gm', reprocess=True)

or 

.. code-block:: bash

  $ arcana derive 'myuni-xnat//myproject:training' freesurfer/recon-all  --reprocess


To ingore differences between pipeline versions you can use the ``ignore``
method

.. code-block:: python

  dataset.ignore('freesurfer_pipeline', ('freesurfer_task', 'num_iterations', 3))

or via the CLI

.. code-block:: bash

  $ arcana ignore 'myuni-xnat//myproject:training' freesurfer --param freesurfer_task num_iterations 3



.. _Pydra: http://pydra.readthedocs.io