.. _processing:

Processing
==========

Derivatives are generated in Arcana by "pipelines", modular Pydra workflows
that connect dataset columns (see :ref:`data_columns`). Pipeline outputs
are connected to sink columns, while pipeline inputs can draw data from either
source columns or sink columns containing derivatives generated by prerequisite
pipelines.


Pipelines
---------

Pipeline objects wrap around the "inner" workflow, which can consist of a
single Pydra task or a series of processing steps in a Pydra workflow to
process/analyse the data. The "outer" workflow implicitly created by the
pipeline object, adds tasks to iterate over data nodes, handle storage and
retrieval to and from the data store, convert data between file formats and
management of provenance and previously generated derivatives.

Connecting a pipeline via API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To connect a workflow between columns of a dataset via the API use the 
``Dataset.pipeline()`` method

.. code-block:: python

    from pydra.tasks.freesurfer import Freesurfer
    from arcana.data.types import general, medicalimaging

    dataset = Dataset.load('myuni-xnat//myproject:training')

    dataset.add_source('T1w', medicalimaging.dicom, path='.*mprage.*',
                       is_regex=True)
    dataset.add_source('T2w', medicalimaging.dicom, path='.*t2spc.*',
                       is_regex=True)

    dataset.add_sink('freesurfer/recon-all', general.directory)

    dataset.pipeline(
        name='freesurfer,
        workflow=Freesurfer(
            param1=10.0,
            param2=20.0),
        inputs=[('T1w', 'in_file', medicalimaging.nifti_gz),
                ('T2w', 'peel', medicalimaging.nifti_gz)],
        outputs=[('freesurfer/recon-all', 'out_file', general.directory)])

Alternatively, if the source can be referenced by its path alone you can
add the sources and sinks in one step

.. code-block:: python

    from pydra.tasks.fsl.preprocess.fast import FAST
    from arcana.data.types import general, medicalimaging

    dataset = Dataset.load('file///data/openneuro/ds00014:test')

    dataset.pipeline(
        name='segmentation',
        workflow=FAST(
            method='a-method'),
        sources=[('T1w', 'in_file', medicalimaging.nifti_gz)],
        sinks=[('fast/gm', 'gm', medicalimaging.nifti_gz)])

      # Save pipeline to dataset metadata for subsequent reuse.
      dataset.save()


Connecting a pipeline via CLI
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To connect a workflow via the CLI

.. code-block:: bash

    $ arcana column add-source 'myuni-xnat//myproject:training' T1w \
      medicalimaging:dicom --path '.*mprage.*'
    $ arcana column add-source 'myuni-xnat//myproject:training' T2w \
      medicalimaging:dicom --path '.*t2spc.*'
    $ arcana pipeline 'myuni-xnat//myproject:training' freesurfer \
      pydra.tasks.freesurfer:Freesurfer \
      --input T1w in_file medicalimaging:nifti_gz \
      --input T2w peel medicalimaging:nifti_gz \
      --output freesurfer/recon-all out_file general:directory \
      --parameter param1 10.0 \
      --parameter param2 20.0

Or adding sinks and sources in one step where they can be specified by their
path and datatype alone

.. code-block:: bash

    $ arcana pipeline 'file///data/openneuro/ds00014:test' segmentation \
      pydra.tasks.fsl.preprocess.fast:FAST \
      --source T1w in_file medicalimaging:nifti_gz \
      --sink fast/gm gm medicalimaging:nifti_gz \
      --parameter method a-method


Derivatives
-----------

After data sinks have been defined and connected to a pipeline, they can be
generated using the ``derive`` method on the dataset. This method checks the
dataset to see whether the source data is present and executes the
pipelines over all nodes of the dataset with available source data by default.
If pipeline inputs are sink columns to be derived by prerequisite pipelines,
then the prerequisites are executed first.


Generating derivatives via API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

  dataset = Dataset.load('file///data/openneuro/ds00014:test')

  dataset.derive('fast/gm')

  # Print URI of generated dataset
  print(dataset['fast/gm']['sub11'].uri)


Generating derivatives via CLI
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: bash

  $ arcana derive 'myuni-xnat//myproject:training' freesurfer/recon-all


Under the hood, Arcana uses the Pydra workflow engine to execute the 

Provenance
----------

Provenance metadata is saved alongside derivatives in the data store. The
metadata includes:

* MD5 Checksums of all pipeline inputs and outputs
* Full workflow graph with connections between, and parameterisations of, Pydra tasks
* Container image tags for tasks that ran inside containers
* Python dependencies and versions used.

Before derivatives are generated, the provenance metadata of prerequisite
derivatives (i.e. inputs of the pipeline and prerequisite pipelines, etc...)
are checked to see if there have been any alterations to the configuration of
the pipelines that generated them. If so, any affected nodes will not be
processed, and a warning will be generated, unless the ``reprocess`` flag is
set

.. code-block:: python

  dataset.derive('fast/gm', reprocess=True)

or 

.. code-block:: bash

  $ arcana derive 'myuni-xnat//myproject:training' freesurfer/recon-all  --reprocess


To ingore differences between pipeline versions you can use the ``ignore``
method

.. code-block:: python

  dataset.ignore('freesurfer', ('param', 10.0))
