Processing
==========

All data processing in Arcana is performed by the Pydra_ dataflow engine, which
defines a declarative workflow language and enables the execution of workflows
to be split over multiple cores or high-performance clusters (i.e. SLURM and SGE).

Processed derivatives are computed by "pipelines", modular Pydra_ workflows
that connect dataset columns (see :ref:`data_columns`). Pipeline outputs are
always connected to sink columns, whereas pipeline inputs can draw data from either
source columns or sink columns containing derivatives generated by prerequisite
pipelines.

By connecting pipeline inputs to the outputs of other pipelines,
complex processing chains/webs can be created (reminiscent of a makefile),
in which intermediate products will be stored in the dataset for subsequent
analysis. Alternatively, :class:`.Analysis` classes can be used to implement
processing webs independently of a specific dataset so that they can be applied
to multiple studies in a reproducible way. If required, :class:`.Analysis`
classes can be customised for particular use cases via combination of new
parameterisations and overriding selected methods in subclasses (see :ref:`design_analyses`).

.. note::

  While a general-purpose dataflow engine, Pydra_ was developed in the neuroimaging
  field and so the existing task interfaces wrap common neuroimaging tools. Therefore,
  you will need to create your own Pydra_ wrappers for other tools used in other
  domains.

Pipelines
---------

:class:`.Pipeline` objects wrap Pydra_ workflows or individual tasks to prepend/append
tasks to the workflow to

* iterate over the relevant nodes
* manage storage and retrieval of data to and from the data store
* conversion between between different file formats
* write provenance metadata
* check saved provenance metadata to ensure prerequisite derivatives were generated with equivalent parameterisations and software versions, and reprocess them if not

To add a pipeline to a dataset via the API use the :meth:`Dataset.pipeline` method
mapping the inputs and outputs of the Pydra_ workflow/task (``in_file``, ``peel``
and ``out_file`` in the example below) to appropriate columns in the dataset
(``T1w``, ``T2w`` and ``freesurfer/recon-all`` respectively)

.. code-block:: python

    from pydra.tasks.freesurfer import Freesurfer
    from arcana.data.formats import common, medicalimaging

    dataset = Dataset.load('myuni-xnat//myproject:training')

    dataset.add_source('T1w', format=medicalimaging.dicom, path='.*mprage.*',
                       is_regex=True)
    dataset.add_source('T2w', format=medicalimaging.dicom, path='.*t2spc.*',
                       is_regex=True)

    dataset.add_sink('freesurfer/recon-all', common.directory)

    dataset.pipeline(
        name='freesurfer,
        workflow=Freesurfer(
            param1=10.0,
            param2=20.0),
        inputs=[('T1w', 'in_file', medicalimaging.nifti_gz),
                ('T2w', 'peel', medicalimaging.nifti_gz)],
        outputs=[('freesurfer/recon-all', 'out_file', common.directory)])

If there is a mismatch in the data format (see :ref:`data_formats`) between the
workflow inputs/outputs and the columns they are connected to, the pipeline will
insert a task to perform the conversion if a converter method between the two
formats exists.

If the source can be referenced by its path alone, then you can add the sources
and sinks in one step

.. code-block:: python

    from pydra.tasks.fsl.preprocess.fast import FAST
    from arcana.data.formats import common, medicalimaging

    dataset = Dataset.load('file///data/openneuro/ds00014:test')

    dataset.pipeline(
        name='segmentation',
        workflow=FAST(
            method='a-method'),
        sources=[('T1w', 'in_file', medicalimaging.nifti_gz)],
        sinks=[('fast/gm', 'gm', medicalimaging.nifti_gz)])

    # Save pipeline to dataset metadata for subsequent reuse.
    dataset.save()


To connect a workflow via the CLI

.. code-block:: bash

    $ arcana column add-source 'myuni-xnat//myproject:training' T1w \
      medicalimaging:dicom --path '.*mprage.*'
    $ arcana column add-source 'myuni-xnat//myproject:training' T2w \
      medicalimaging:dicom --path '.*t2spc.*'
    $ arcana pipeline 'myuni-xnat//myproject:training' freesurfer \
      pydra.tasks.freesurfer:Freesurfer \
      --input T1w in_file medicalimaging:nifti_gz \
      --input T2w peel medicalimaging:nifti_gz \
      --output freesurfer/recon-all out_file common:directory \
      --parameter param1 10 \
      --parameter param2 20

Adding sinks and sources in one step where they can be specified by their
path and format alone looks like

.. code-block:: bash

    $ arcana pipeline 'file///data/openneuro/ds00014:test' segmentation \
      pydra.tasks.fsl.preprocess.fast:FAST \
      --source T1w in_file medicalimaging:nifti_gz \
      --sink fast/gm gm medicalimaging:nifti_gz \
      --parameter method a-method


By default, pipelines will iterate all leaf nodes of the data tree (e.g. ``session``
for datasets in the :class:`.ClinicalTrial` space). However, pipelines can be run
at any row frequency of the dataset, e.g. per subject, per timepoint, or on the
dataset as a whole (to create single templates/statistics).

Pipeline outputs must be connected to sinks of the same row frequency. However,
inputs can be drawn from columns of any row frequency. In this case,
inputs from more frequent nodes will be provided to the pipeline as a list
sorted by their ID. 

For example, when the pipeline in the following code-block runs, it will receive
a list of T1w filenames, run one workflow node, and then sink a single template
back to the dataset.


.. code-block:: python

    from myworkflows import vbm_template
    from arcana.data.formats import common, medicalimaging
    from arcana.data.spaces.medicalimaging import ClinicalTrial

    dataset = Dataset.load('bids///data/openneuro/ds00014')

    # Add sink column with `dataset` row frequency
    dataset.add_sink(
      name='vbm_template',
      format=medicalimaging.nifti_gz
      frequency=ClinicalTrial.dataset
    )

    # Connect pipeline to `dataset` row frequency sink column. Needs to be
    # of `dataset` frequency itself or Arcana will raise an error
    dataset.pipeline(
        name='vbm_template',
        workflow=vbm_template(),
        inputs=[('in_file', 'T1w')],
        outputs=[('out_file', 'vbm_template')],
        frequency=ClinicalTrial.dataset)


Analysis classes
----------------

Analysis classes can be used to implement pipeline chains that are independent of
specific datasets. 

Analysis classes are specified using a domain-specific language based on the
`attrs <https://www.attrs.org/en/stable/>`_ package (see `https://www.attrs.org/en/stable/extending.html`_).


.. code-block:: python

  from pydra.tasks.mrtrix3.preprocess import FslPreproc
  from arcana.core.mark import Pipeline, analysis, column, pipeline
  from arcana.core.enum import DataSalience as ds
  from arcana.data.formats.medicalimaging import (
    DwiImage, NiftiGzXD, MrtrixIF, MrtrixTF)


  @analysis
  class DwiAnalysis():

      # Define the columns for the dataset.
      dw_images: DwiImage = column(
        "Reconstructed diffusion-weighted images acquired from scanner",
        salience=ds.primary)
      reverse_phase: DwiImage = column(
        "Reverse-phase encoded used to correct for phase-encoding distortions",
        salience=ds.primary)
      preprocessed: NiftiGzXD = column(
        "Preprocesed and corrected diffusion-weighted images", salience=ds.debug)
      wm_odf: MrtrixIF = column(
        "White matter orientation distributions", salience=ds.debug)
      afd: MrtrixIF = column(
        "Apparent fibre orientations", salience=ds.publication)
      global_tracks: MrtrixTF = column(
        "Tracking of white matter tracts across brain", salience=ds.publication)

      # Define a pipeline constructor method to generate the 'preprocessed'
      # derivative.
      @pipeline(preprocessed)
      def preprocess(self,
                     pipeline: Pipeline,
                     dw_images: NiftiGzXD,
                     reverse_phase: NiftiGzXD):

          # Add tasks to the pipeline using Pydra workflow syntax
          pipeline.add(
            name='preprocess',
            task=FslPreproc(
              in_file=dwi_images
              reverse_phase=reverse_phase))

          pipeline.set_output(('preprocessed', pipeline.preprocess.out_file))
      
      




Derivatives
-----------

After pipelines have been connected to a sink column, the its data can be
generated using :meth:`.Dataset.derive`. This method checks the
dataset to see whether the source data is present and executes the
pipelines over all nodes of the dataset with available source data by default.
If pipeline inputs are sink columns to be derived by prerequisite pipelines,
then the prerequisite pipelines will be prepended onto the pipeline stack.

To generate derivatives via the API

.. code-block:: python

  dataset = Dataset.load('file///data/openneuro/ds00014:test')

  dataset.derive('fast/gm', work_dir='/work/temp-dir')

  # Print URI of generated dataset
  print(dataset['fast/gm']['sub11'].uri)


To generate derivatives via the CLI

.. code-block:: bash

  $ arcana derive 'myuni-xnat//myproject:training' freesurfer/recon-all


By default Pydra_ uses the "concurrent-futures" (`'cf'`) plugin, which
splits workflows over multiple processes. You can specify which plugin, and
thereby how the workflow is executed via the ``pydra_plugin`` option, and pass
options to it with ``pydra_option``.


.. code-block:: bash

  $ arcana derive 'myuni-xnat//myproject:training' freesurfer/recon-all \
    --pydra_plugin slurm --pydra_option poll_delay 5 --pydra_option max_jobs 10


Provenance
----------

Provenance metadata is saved alongside derivatives in the data store. The
metadata includes:

* MD5 Checksums of all pipeline inputs and outputs
* Full workflow graph with connections between, and parameterisations of, Pydra tasks
* Container image tags for tasks that ran inside containers
* Python dependencies and versions used.

How these provenance metadata are stored will depend on the type data store,
but often it will be stored in a JSON file. For example, a provenance JSON file
would look like

.. code-block:: javascript

  {
    "store": {
      "type": "xnat",
      "location": "https://central.xnat.org"
    },
    "dataset": {
      "id": "MYPROJECT",
      "name": "training"
    },
    "checksums": {
      "inputs": {
        // MD5 Checksums for all files in the file group. "." refers to the
        // "primary file" in the file group.
        "T1w_reg_dwi": {
          ".": "4838470888DBBEADEAD91089DD4DFC55",
          "json": "7500099D8BE29EF9057D6DE5D515DFFE"
        },
        "T2w_reg_dwi": {
          ".": "4838470888DBBEADEAD91089DD4DFC55",
          "json": "5625E881E32AE6415E7E9AF9AEC59FD6"
        },
        "dwi_fod": {
          ".": "92EF19B942DD019BF8D32A2CE2A3652F"
        }
      },
      "outputs": {
        "wm_tracks": {
          ".": "D30073044A7B1239EFF753C85BC1C5B3"
        }
      }
    },
    "pipeline": {
      "name": "anatomically_constrained_tractography",
      // List all tasks in the pipeline and the inputs to them. 
      "tasks": [
        {
          "name": "5ttgen",
          "task": {
            "module": "pydra.tasks.mrtrix3.preprocess",
            "name": "FiveTissueTypes",
            "package": "pydra-mrtrix",
            "version": "0.1.1"
          }
          "inputs": {
            "in_file": {
              "field": "T1w_reg_dwi"
            }
            "t2": {
              "field": "T1w_reg_dwi"
            }
            "sgm_amyg_hipp": true
          },
          "image": {
            "type": "docker",
            "tag": "mrtrix3/mrtrix3"
          }
        },
        {
          "name": "tckgen",
          "task": {
            "module": "pydra.tasks.mrtrix3.tractography",
            "name": "TrackGen",
            "package": "pydra-mrtrix",
            "version": "0.1.1"
          }
          "inputs": {
            "in_file": {
              "field": "dwi_fod"
            },
            "act": {
              "task": "5ttgen",
              "field": "out_file"
            },
            "select": 100000000,
          },
          "image": {
            "type": "docker",
            "tag": "mrtrix3/mrtrix3"
          }
        }
      ],
      "outputs": {
        "wm_tracks": {
          "task": "tckgen",
          "field": "out_file"
        }
      }
    }
  }


Before derivatives are generated, provenance metadata of prerequisite
derivatives (i.e. inputs of the pipeline and prerequisite pipelines, etc...)
are checked to see if there have been any alterations to the configuration of
the pipelines that generated them. If so, any affected nodes will not be
processed, and a warning will be generated, unless the ``reprocess`` flag is
set

.. code-block:: python

  dataset.derive('fast/gm', reprocess=True)

or 

.. code-block:: bash

  $ arcana derive 'myuni-xnat//myproject:training' freesurfer/recon-all  --reprocess


To ingore differences between pipeline versions you can use the ``ignore``
method

.. code-block:: python

  dataset.ignore('freesurfer_pipeline', ('freesurfer_task', 'num_iterations', 3))

or via the CLI

.. code-block:: bash

  $ arcana ignore 'myuni-xnat//myproject:training' freesurfer --param freesurfer_task num_iterations 3



.. _Pydra: http://pydra.readthedocs.io